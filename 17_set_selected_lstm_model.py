# -*- coding: utf-8 -*-
"""17 set selected lstm model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HXZzxd97RLcVPrnwiF25zBZqRUEcZIui
"""
#####Installing the Google colab plugin for file upload #####################
!pip install -U -q PyDrive

import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# choose a local (colab) directory to store the data.
local_download_path = os.path.expanduser('~/data')
try:
  os.makedirs(local_download_path)
except: pass

# 2. Auto-iterate using the query syntax

#for model importing

#    https://developers.google.com/drive/v2/web/search-parameters
file_list = drive.ListFile(
    {'q': "'1yWMyOSfajIFBNMh8yILm6CZKV2Fh_zKZ' in parents"}).GetList()

for f in file_list:
  # 3. Create & download by id.
  print('title: %s, id: %s' % (f['title'], f['id']))
  fname = os.path.join(local_download_path, f['title'])
  print('downloading to {}'.format(fname))
  f_ = drive.CreateFile({'id': f['id']})
  f_.GetContentFile(fname)

#for faults  importing

#    https://developers.google.com/drive/v2/web/search-parameters
file_list = drive.ListFile(
    {'q': "'1ogTVFnFsWdLyJjXlkDC1LVwbWBQM6YFw' in parents"}).GetList()

for f in file_list:
  # 3. Create & download by id.
  print('title: %s, id: %s' % (f['title'], f['id']))
  fname = os.path.join(local_download_path, f['title'])
  print('downloading to {}'.format(fname))
  f_ = drive.CreateFile({'id': f['id']})
  f_.GetContentFile(fname)

#for selected numpy file  importing

#    https://developers.google.com/drive/v2/web/search-parameters
file_list = drive.ListFile(
    {'q': "'1OEznZxWWh8JCEtMUjninL-UwAyBRSLZH' in parents"}).GetList()

for f in file_list:
  # 3. Create & download by id.
  print('title: %s, id: %s' % (f['title'], f['id']))
  fname = os.path.join(local_download_path, f['title'])
  print('downloading to {}'.format(fname))
  f_ = drive.CreateFile({'id': f['id']})
  f_.GetContentFile(fname)
#####End of loaing files for train , validation and test #####################

#####               Importing Libraries                   #####################
from keras.layers import Dense,Dropout
from keras.layers import LSTM
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
import matplotlib.pyplot as plt
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.models import load_model
from sklearn.decomposition import PCA
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
import os.path
import gc
from scipy.signal import savgol_filter

###################load all models in an array #########################
model=[]
model_list=[1,2,3,4,7,8,11,12,13,14,17,18,20,24,25,26,27]
for model_num in range(len(model_list)):
  print(model_num)
  filename='/root/data/fault'+str(model_list[model_num])+'_24_model.h5'
  model.append(load_model(filename))
print("done" , len(model))

#checking gradient change on fault 20


fault_vl=4
window=30
thres=0.4

selected=np.load("/root/data/fault"+str(fault_vl)+"_selectedvariables.npy")
#loading fault
tester=read_csv("/root/data/fault"+str(fault_vl)+"_result_processed.csv",index_col=0)
tester=tester.values
tester_X, tester_y = tester[:, :-1], tester[:, -1]

print(len(tester))

yhat_full=[]


for sample in range(window-1,800,window):
  tester_sub=tester_X[(sample-window+1):sample+1,selected]
  tester_sub = tester_sub.reshape((tester_sub.shape[0], 1, tester_sub.shape[1]))  
  #compute prob for each model
  yhat=[]
  for list_num in range(len(model_list)):
    yhat.append(model[list_num].predict(tester_sub))
#  print(yhat)
 # print(np.array(yhat))
  yhat_np=np.array(yhat)
  #computes 1 d array of predictions of that sample for all 12 models
  yhat_avg=np.mean(yhat_np,axis=1).flatten()
  #appending values of 2 samples to the bigger array for gradient check
  yhat_full.append(yhat_avg) 

 
  
print(np.array(yhat_full).shape)
yhat_full=np.array(yhat_full)

plt.plot(yhat_full[:,0])
plt.plot(yhat_full[:,1])
plt.plot(yhat_full[:,2])
plt.plot(yhat_full[:,3])
plt.plot(yhat_full[:,4])
plt.plot(yhat_full[:,5])
plt.plot(yhat_full[:,6])
plt.plot(yhat_full[:,7])
plt.plot(yhat_full[:,8])
plt.plot(yhat_full[:,9])
plt.plot(yhat_full[:,10])
plt.plot(yhat_full[:,11])
plt.plot(yhat_full[:,12])
plt.plot(yhat_full[:,13])
plt.plot(yhat_full[:,14],'ro')
plt.plot(yhat_full[:,15])
plt.plot(yhat_full[:,16])


plt.show()

def update_score(solution,checker,matrix):
  sol=np.array(solution,dtype='int32')
  back_score=solution[2]
  for_score=0
  sample=np.int(sol[1]/window)
  for i in range(sample,checker):
    fault_index=np.where(model_list==sol[0])
    for_score=for_score+np.square(1-yhat_full[sample,fault_index])
  for_score=for_score/(checker-sample)
  print("model|      "+str(sol[0])+"      |cp| "+str(sol[1])+"     |updated| "+str(for_score+back_score))
  return (for_score+back_score)
  
   

def verify_fault_bfr(matrix,model_num,point_detected):
  sum1=0
  for i in range(0,point_detected):
    sum1=sum1+np.square(matrix[i,model_num])
  sum1=sum1/point_detected  
  return sum1
  
  
solution=np.zeros((1000,3))
row=0
for checker in range(4,np.size(yhat_full,0)):
  
  
  print(checker*window)
  
  for rw in range(row):
    solution[rw,2]=update_score(solution[rw,:],checker,yhat_full)
    
    
  for mdl in range(len(model_list)):
    comp1=yhat_full[checker,mdl]-yhat_full[(checker-1),mdl]
    comp2=yhat_full[checker,mdl]-yhat_full[(checker-2),mdl]
    comp3=yhat_full[checker,mdl]-yhat_full[(checker-3),mdl]
    comp4=yhat_full[checker,mdl]-yhat_full[(checker-4),mdl]
    
    if(comp1 >thres or comp2>thres or comp3>thres or comp4>thres):
        solution[row,0]=model_list[mdl]
        solution[row,1]=(checker-1)*window
        solution[row,2]=verify_fault_bfr(yhat_full,mdl,checker)
        
        #print("fault maybe in"+ str(solution[row,0]))
        #print("detection point"+str(solution[row,1]))       
        #print("MSE with previous values"+str(solution[row,2]))
        row=row+1
  print("ending this samples calculatin",'/n','/n')
